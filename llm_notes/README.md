# LLM Notes

This folder contains comprehensive learning materials and notes about Large Language Models (LLMs).

## 目录结构 (Directory Structure)

### 1. 大模型整体架构 (Overall Architecture of Large Language Models)
- **code/**: Implementation examples of transformer architecture, attention mechanisms, etc.
- **questions/**: Questions and discussions about model architecture

### 2. 预训练 (Pre-training)
- **code/**: Pre-training implementation examples, data processing, training loops
- **questions/**: Questions and discussions about pre-training process

### 3. 后训练 (Post-training / Fine-tuning)
- **code/**: Fine-tuning examples, LoRA, RLHF, etc.
- **questions/**: Questions and discussions about fine-tuning and post-training

### 4. 常见模型 (Common Models)
- **code/**: Code examples for GPT, BERT, T5, LLaMA, etc.
- **questions/**: Questions and discussions about specific models

## 使用说明 (Usage)

Each topic folder contains:
- `code/`: Practical coding examples and implementations
- `questions/`: Questions, discussions, and notes
- `README.md`: Detailed explanations and resources

## 学习路径 (Learning Path)

1. Start with **大模型整体架构** to understand the fundamentals
2. Move to **预训练** to learn how models are initially trained
3. Study **后训练** for fine-tuning and adaptation techniques
4. Explore **常见模型** to understand specific implementations

