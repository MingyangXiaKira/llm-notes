# 大模型整体架构 (Overall Architecture of Large Language Models)

## 核心内容 (Core Topics)

### 1. Transformer 架构
- Self-Attention Mechanism
- Multi-Head Attention
- Position Encoding
- Feed-Forward Networks
- Layer Normalization

### 2. 编码器-解码器架构
- Encoder-only models (BERT)
- Decoder-only models (GPT)
- Encoder-Decoder models (T5, BART)

### 3. 注意力机制详解
- Scaled Dot-Product Attention
- Masked Self-Attention
- Cross-Attention

### 4. 模型组件
- Embedding Layer
- Positional Encoding
- Transformer Blocks
- Output Layer

## 代码示例 (Code Examples)

See `code/` folder for:
- Transformer implementation from scratch
- Attention mechanism visualization
- Model architecture diagrams

## 问题与讨论 (Questions and Discussions)

See `questions/` folder for:
- Architecture design questions
- Attention mechanism explanations
- Trade-offs between different architectures

